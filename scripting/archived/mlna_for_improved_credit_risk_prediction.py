# -*- coding: utf-8 -*-
"""mlna_for_improved_credit_risk_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GroqwAr6b1ym9Id12p-qFq-U6sPg0RCm

<div style="background-color: #2020D1; padding: 40px; border-radius: 10px; color: #FFFFFF; text-align: center; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
    <h1 style="font-size: 48px; font-weight: bold;">üè¶ MULTI-LAYER NEURAL NETWORK CREDIT RISK PREDICTION </h1>
    <p style="font-size: 24px; font-weight: bold; margin-top: 20px;">Precision: 0.XX &nbsp;&nbsp; Accuracy: 0.YY</p>
    <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTitfBKMcp-K-Yu_9wTbUVAro0EzqSyCVIVUw&usqp=CAU" alt="GIF" style="width: 400px; margin: 20px auto;">
</div>

<a id='top'></a>
<div class="list-group" id="list-tab" role="tablist"></div>

<!-- Bank Money Theme -->
<div style="border-radius: 0px; border: 2px solid #2020ff; padding: 15px; background-color: #2020d1; font-size: 120%; text-align: center; color: #FFFFFF; font-weight: bold;">
   Table of Contents
</div>


* [1. üíæ Import Libraries](#1.-Import-Libraries)

* [2. üìë Read and Explain Dataset](#2.-Read-and-Explain-Dataset)

* [3. üìä Exploratory Data Analysis](#3.-Exploratory-Data-Analysis)

* [4. üéØ Dealing with Outliers](#4.-Dealing-with-Outliers)

* [5. üí° Feature Engineering](#5.-Feature-Engineering)

* [6. üõ†Ô∏è Data Preprocessing](#6.-Data-Preprocessing)

* [7. üå¥ ML Classification Models](#7.-ML-Classification-Models)

* [8. ‚öôÔ∏è Hyperparameter Tuning](#8.-Hyperparameter-Tuning)

* [9. üöÄ Training Final Model](#9.-Training-Final-Model)

* [10. ü§ñ Build Multi layer Graph](#10.-Build-Multi-layer-Graph)

## <div style="border-radius:0px; border:#2020ff solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">1. Import Libraries</div>
"""

#!pip install tensorflow
#!pip install scipy

# Commented out IPython magic to ensure Python compatibility.
# data
import pandas as pd
import numpy as np


# visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go

# styling
# %matplotlib inline
sns.set_style('darkgrid')
mpl.rcParams['font.size'] = 14
mpl.rcParams['figure.facecolor'] = '#00000000'
mpl.rcParams['font.size'] = 14
mpl.rcParams['figure.facecolor'] = '#00000000'

import os

import warnings
warnings.filterwarnings("ignore")

"""## <div style="border-radius:0px; border:#2020ff solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">2. Read and Explain Dataset</div>"""

#Read data
data_original =  pd.read_csv("https://raw.githubusercontent.com/VictorNico/INFO4017_22_23_UYI/DS/credit_risk_dataset.csv")
#data_original =  pd.read_csv("./datasets/credit_risk_dataset.csv")
data = data_original.copy()


# Print sample
data.sample(6).reset_index(drop=True).style.set_properties(**{'background-color': '#2020d1','color': '#FFFFFF','border-color': '#FFFFFF'})

"""<div style="background-color: #2020d1; padding: 10px; border-radius: 10px; color: #FFFFFF; text-align: center; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
    <h1 style="font-size: 24px;">Feature Descriptions</h1>
</div>

<div style="background-color: #FFFFFF; padding: 15px; border-radius: 10px; margin-top: 20px; text-align: left;; color: #2026d1">
    <ul>
        <li><b>person_age:</b> Age of the individual applying for the loan.</li>
        <li><b>person_income:</b> Annual income of the individual.</li>
        <li><b>person_home_ownership:</b> Type of home ownership of the individual.
            <ul>
                <li>rent: The individual is currently renting a property.</li>
                <li>mortgage: The individual has a mortgage on the property they own.</li>
                <li>own: The individual owns their home outright.</li>
                <li>other: Other categories of home ownership that may be specific to the dataset.</li>
            </ul>
        </li>
        <li><b>person_emp_length:</b> Employment length of the individual in years.</li>
        <li><b>loan_intent:</b> The intent behind the loan application.</li>
        <li><b>loan_grade:</b> The grade assigned to the loan based on the creditworthiness of the borrower.
            <ul>
                <li>A: The borrower has a high creditworthiness, indicating low risk.</li>
                <li>B: The borrower is relatively low-risk, but not as creditworthy as Grade A.</li>
                <li>C: The borrower's creditworthiness is moderate.</li>
                <li>D: The borrower is considered to have higher risk compared to previous grades.</li>
                <li>E: The borrower's creditworthiness is lower, indicating a higher risk.</li>
                <li>F: The borrower poses a significant credit risk.</li>
                <li>G: The borrower's creditworthiness is the lowest, signifying the highest risk.</li>
            </ul>
        </li>
        <li><b>loan_amnt:</b> The loan amount requested by the individual.</li>
        <li><b>loan_int_rate:</b> The interest rate associated with the loan.</li>
        <li><b>loan_status:</b> Loan status, where 0 indicates non-default and 1 indicates default.
            <ul>
                <li>0: Non-default - The borrower successfully repaid the loan as agreed, and there was no default.</li>
                <li>1: Default - The borrower failed to repay the loan according to the agreed-upon terms and defaulted on the loan.</li>
            </ul>
        </li>
        <li><b>loan_percent_income:</b> The percentage of income represented by the loan amount.</li>
        <li><b>cb_person_default_on_file:</b> Historical default of the individual as per credit bureau records.
            <ul>
                <li>Y: The individual has a history of defaults on their credit file.</li>
                <li>N: The individual does not have any history of defaults.</li>
            </ul>
        </li>
        <li><b>cb_preson_cred_hist_length:</b> The length of credit history for the individual.</li>
    </ul>
</div>

## <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">3. Exploratory Data Analysis</div>
"""

data.shape

data.head()

data.tail()

data.describe()

data.info()

data.isnull().sum()

data.nunique()

data.duplicated().sum()

data.corr()

fig, ax = plt.subplots()
fig.set_size_inches(15,8)
sns.heatmap(data.corr(), vmax =.8, square = True, annot = True,cmap='Blues' )
plt.title('Confusion Matrix',fontsize=15);

"""!pip install dython

#Study correlation between categorial features
from dython.nominal import associations
from dython.nominal import identify_nominal_columns
categorical_features=identify_nominal_columns(data)
categorical_features

#generate correlation matrix
associations(dataset, nominal_columns='auto', numerical_columns=None, mark_columns=False, nom_nom_assoc='cramer', num_num_assoc='pearson', bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE, ax=None, figsize=None, annot=True, fmt='.2f', cmap=None, sv_color='silver', cbar=True, vmax=1.0, vmin=None, plot=True, compute_only=False, clustering=False, title=None, filename=None)

#get a full data correlation between features
complete_correlation= associations(data, filename= 'complete_correlation.png', figsize=(10,10))
df_complete_corr=complete_correlation['corr']
df_complete_corr.dropna(axis=1, how='all').dropna(axis=0, how='all').style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)
"""

from IPython.core.display import HTML

#Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell
def multi_table(table_list):
        return HTML('<table><tr style="background-color:#2020d1; color: #FFFFFF;">' +  ''.join(['<td>' + table._repr_html_() + '</td>' for table in table_list]) +'</tr></table>')

nunique_df={var:pd.DataFrame(data[var].value_counts())
           for var in {'person_age', 'person_income', 'person_home_ownership',
       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',
       'loan_int_rate', 'loan_status', 'loan_percent_income',
       'cb_person_default_on_file', 'cb_person_cred_hist_length'}}

multi_table([nunique_df['person_age'],nunique_df['person_income'],nunique_df['person_home_ownership'],nunique_df['person_emp_length'],nunique_df['loan_intent'],nunique_df['loan_grade'],nunique_df['loan_amnt'],nunique_df['loan_int_rate'],nunique_df['loan_status'],nunique_df['loan_percent_income'],nunique_df['cb_person_default_on_file'],nunique_df['cb_person_cred_hist_length']])

"""## Univarient Analysis"""

#MAX AND MIN AGE
max_ = data['person_age'].max()
min_ = data['person_age'].min()
print(f"maximum Age {max_}")
print(f"minimum Age {min_}")

# people with an age between x and y
def age_group(arr):
    lenarr = len(arr)
    for i in range(0,lenarr-1):
        next = arr[i]+1
        num_people = data['person_age'].between(next,arr[i+1]).sum()
        print(f'Age between {next} and {arr[i+1]}: Number of people {num_people}')

age_group([0 ,18, 26, 36, 46, 56, 66, float('inf')])

#max and min income
max_ = data['person_income'].max()
min_ = data['person_income'].min()

print(f"maximum Income {max_}")
print(f"minimum Income {min_}")

#people with an income between x and y
def income_group(arr):
    lenarr = len(arr)
    for i in range(0,lenarr-1):
        next = arr[i]+1
        num_people = data['person_income'].between(next,arr[i+1]).sum()
        print(f'Income between {next} and {arr[i+1]}: Number of people {num_people}')

income_group([0, 25000, 50000, 75000, 100000,float('inf')])

#min and max loan amount
max_loan_amount = data['loan_amnt'].max()
min_loan_amount = data['loan_amnt'].min()

print(f"maximum Loan Amount {max_loan_amount}")
print(f"minimum Loan Amount {min_loan_amount}")

# people with an income between x and y
def loan_amount_group(arr):
    lenarr = len(arr)
    for i in range(0,lenarr-1):
        next = arr[i]+1
        num_people = data['loan_amnt'].between(next,arr[i+1]).sum()
        print(f'Loan Amount between {next} and {arr[i+1]}: Number of people {num_people}')

loan_amount_group([0, 5000, 10000, 15000, float('inf')])

level_counts=data.person_home_ownership.value_counts()
fig=px.pie(values=level_counts.values,
          names=level_counts.index,
          color_discrete_sequence=px.colors.sequential.Plasma,
          title= 'person_home_ownership'
          )
fig.update_traces(textinfo='label+percent+value', textfont_size=13,
                  marker=dict(line=dict(color='#2020d1', width=0.2)))

fig.data[0].marker.line.width = 2
fig.data[0].marker.line.color='white'
fig.show()

"""**Observations: Most of the People taking a loan doesnt own their own house**

"""

level_counts=data.loan_grade.value_counts()
fig=px.pie(values=level_counts.values,
          names=level_counts.index,
          color_discrete_sequence=px.colors.sequential.Plasma,
          title= 'loan_grade'
          )
fig.update_traces(textinfo='label+percent+value', textfont_size=13,
                  marker=dict(line=dict(color='#2020d1', width=0.2)))

fig.data[0].marker.line.width = 2
fig.data[0].marker.line.color='white'
fig.show()

"""**Observations: Loan Grade A and B comprises of 65.2% of total**"""

fig=px.histogram(data, x = 'loan_intent',histnorm = 'percent', text_auto = '.2f',template = 'presentation', title = 'loan intent',color_discrete_sequence=px.colors.sequential.Plasma)
fig.update_layout()
fig.show()

"""**Observations:Loan Intent are equally almost distributed**"""

fig=px.histogram(data, x = 'cb_person_cred_hist_length', text_auto = '.2f',template = 'presentation', title = 'person credit history length',color_discrete_sequence=px.colors.sequential.Plasma)
fig.update_layout()
fig.show()

sns.pairplot(data,hue="loan_status")

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">4. Dealing with Outliers</div>"""

data.isnull().sum()

data['person_emp_length'] = data['person_emp_length'].fillna(data['person_emp_length'].quantile(0.5))
data['loan_int_rate'] = data['loan_int_rate'].fillna(data['loan_int_rate'].quantile(0.5))

# drop values that are null
# data.dropna(axis=0,inplace=True)

data.isnull().sum()

data['loan_status'].value_counts()

data.describe()

#person_age max 144 (issue)
#person_emp_length max 123 (issue)
data.reset_index(inplace = True)

import matplotlib.pyplot as plt
verti = data['person_age'].value_counts().values
hori = data['person_age'].value_counts().index
fig = plt.figure(figsize = (15, 5))
plt.bar(hori, verti)
# after 80 it is rare

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">5. Feature Engineering</div>"""

data['age_group'] = pd.cut(data['person_age'],
                           bins=[19, 26, 36, 46, 56, 66, 145],
                           labels=['19-25', '26-35', '36-45', '46-55', '56-65', '66-144'])

data['age_group'].dtype

data.head()

"""## **Income Group**"""

data['income_group'] = pd.cut(data['person_income'],
                              bins=[0, 25000, 50000, 75000, 100000, float('inf')],
                              labels=['low', 'low-middle', 'middle', 'high-middle', 'high'])

data['income_group']

data.head()

"""## **Loan Amount**"""

data['loan_amount_group'] = pd.cut(data['loan_amnt'],
                                   bins=[0, 5000, 10000, 15000, float('inf')],
                                   labels=['small', 'medium', 'large', 'very large'])

data['loan_amount_group']

data = data.drop(data[data['person_emp_length'] > 60].index, axis=0)

data = data.drop(['index'], axis=1)
data.reset_index(inplace = True)
data = data.drop(['index'], axis=1)

data['person_home_ownership'].value_counts()

data.head()

# Create loan-to-income ratio
data['loan_to_income_ratio'] = data['loan_amnt'] / data['person_income']

# Create loan-to-employment length ratio
data['loan_to_emp_length_ratio'] =  data['person_emp_length']/ data['loan_amnt']

# Create interest rate-to-loan amount ratio
data['int_rate_to_loan_amt_ratio'] = data['loan_int_rate'] / data['loan_amnt']

data.columns

col_list = [
    'person_age',
    'person_income',
    'person_home_ownership',
    'person_emp_length',
    'loan_intent',
    'loan_grade',
    'loan_amnt',
    'loan_int_rate',
    'loan_status',
    'loan_percent_income',
    'cb_person_default_on_file',
    'cb_person_cred_hist_length',
    'age_group',
    'income_group',
    'loan_amount_group',
    'loan_to_income_ratio',
    'loan_to_emp_length_ratio',
    'int_rate_to_loan_amt_ratio']


ordinal_factor_colums = [
    'loan_grade',
    'income_group',
    'loan_amount_group']

nominal_factor_colums = [
    'person_home_ownership',
    'loan_intent',
    'cb_person_default_on_file',
    'age_group']

numeric_uniform_colums = [
    'loan_amnt',
    'loan_int_rate',
    'person_income',
    'loan_percent_income',
    'loan_to_income_ratio',
    'loan_to_emp_length_ratio',
    'int_rate_to_loan_amt_ratio']

numeric_with_outliers_columns = [
    'person_age',
    'person_emp_length',
    'cb_person_cred_hist_length']

check = ordinal_factor_colums.copy()
check.extend(nominal_factor_colums)
check.extend(numeric_uniform_colums)
check.extend(numeric_with_outliers_columns)
print("Good") if len(list(set(col_list)-set(check)) if len(check) <= len(col_list) else list(set(check)-set(col_list))) == 1 else print(f"Error")

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">6. Data Preprocessing</div>

"""

# utils functions
def numeric_standardization_with_outliers(data, variables_list):
    dataframe = data.copy()
    # 1) for each variable
    for var in variables_list:
        # a) compute Q1 and Q3
        Q1 = dataframe[var].quantile(0.25)
        Q3 = dataframe[var].quantile(0.75)
        # b) compute IQR
        IQR = Q3 - Q1
        # c) compute sup and inf
        sup = Q3 + (1.5 * IQR)
        inf = Q1 - (1.5 * IQR)
        for line in dataframe.index.values.tolist():
            # if less than inf
            if dataframe.loc[line, var] < inf:
                dataframe.loc[line, var] = inf/sup
            # else greater than sup
            elif dataframe.loc[line, var] > sup:
                dataframe.loc[line, var] = 1
            # else
            else:
                dataframe.loc[line, var] = dataframe.loc[line, var]/sup
    return dataframe

def numeric_uniform_standardization(data, variables_list):
    dataframe = data.copy()
    # 1) for each variable
    for var in variables_list:
        # get maximum value
        maxi = dataframe[var].max()
        dataframe[var] = dataframe[var]/maxi
    return dataframe

def ordinal_factor_encoding(data, variables_list):
    dataframe = data.copy()
    from sklearn.preprocessing import LabelEncoder
    # 1) for each variable
    for var in variables_list:
        label_encoder = LabelEncoder()
        dataframe[var] = label_encoder.fit_transform(dataframe[var])
    return dataframe

def nominal_factor_encoding(data, variables_list):
    dataframe = data.copy()
    from sklearn.preprocessing import OneHotEncoder
    ohe = OneHotEncoder()
    ohe.fit(dataframe[variables_list])
    merge_ohe_col = np.concatenate((ohe.categories_)) # list of all new dimension names
    ohe_data = pd.DataFrame(ohe.transform(dataframe[variables_list]).toarray(), columns=merge_ohe_col) # make the one hot encoding and save the result inside a temp source
    dataframe = pd.concat([ohe_data, dataframe], axis=1) #  concat existing and news columns dimensions
    dataframe = dataframe.drop(variables_list, axis=1) # remove all nominal unencoded dimensions
    return (dataframe, ohe.categories_)

data.isnull().sum()

DATA_OHE, OHE = nominal_factor_encoding(data, nominal_factor_colums)

DATA_OHE_LB = ordinal_factor_encoding(DATA_OHE,ordinal_factor_colums)

DATA_OHE_LB_LBU = numeric_uniform_standardization(DATA_OHE_LB,ordinal_factor_colums)

DATA_OHE_LB_LBU_STDU = numeric_uniform_standardization(DATA_OHE_LB_LBU,numeric_uniform_colums)

DATA_OHE_LB_LBU_STDU_STDWO = numeric_uniform_standardization(DATA_OHE_LB_LBU_STDU,numeric_with_outliers_columns)

OHE

original_col = np.concatenate(([
    'income_group',
    'loan_amount_group',
    'loan_to_income_ratio',
    'loan_to_emp_length_ratio',
    'int_rate_to_loan_amt_ratio'], OHE[3]))
original_col

DATA_ORIGIN = DATA_OHE_LB_LBU_STDU_STDWO.drop(original_col, axis=1)
DATA_ORIGIN.reset_index(inplace = True)
DATA_ORIGIN = DATA_ORIGIN.drop(['index'], axis=1)

DATA_ORIGIN.columns

DATA_OHE_LB_LBU_STDU_STDWO.describe()

from sklearn.model_selection import train_test_split
def test_train(dataframe, target, test_size=0.2, random_state=12):
    X = dataframe.drop([target], axis=1)
    Y = dataframe[target]

    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=test_size,random_state=random_state)

    x_train.reset_index(inplace = True)
    x_test.reset_index(inplace = True)

    x_train = x_train.drop(['index'], axis=1)
    x_train.reset_index(inplace = True)
    x_train = x_train.drop(['index'], axis=1)

    x_test = x_test.drop(['index'], axis=1)
    x_test.reset_index(inplace = True)
    x_test = x_test.drop(['index'], axis=1)

    return x_train, x_test, y_train, y_test

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">7. ML Classification Models</div>

"""

#from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
#from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
#from sklearn.ensemble import AdaBoostClassifier
#from sklearn.ensemble import BaggingClassifier
#from sklearn.ensemble import ExtraTreesClassifier
#from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
#from catboost import CatBoostClassifier
#import lightgbm as lgb
#from lightgbm import LGBMRegressor
from sklearn.metrics import make_scorer, mean_absolute_error
from sklearn.metrics import mean_squared_error as MSE
from hyperopt import hp, fmin, tpe
from sklearn.model_selection import GridSearchCV, StratifiedKFold
#from bayes_opt import BayesianOptimization
from lightgbm import LGBMClassifier

svc = SVC()
knc = KNeighborsClassifier() #algorithm='ball_tree', leaf_size=10, n_neighbors=18, p=1, weights='distance'
#mnb = MultinomialNB()
dtc = DecisionTreeClassifier()
lrc = LogisticRegression()
rfc = RandomForestClassifier()
#abc = AdaBoostClassifier()
#bc = BaggingClassifier()
#etc = ExtraTreesClassifier()
#gbdt = GradientBoostingClassifier()
xgb = XGBClassifier()
#cat = CatBoostClassifier(depth=7, iterations=300, l2_leaf_reg= 1, learning_rate= 0.1,verbose=0) #
#lgb = lgb.LGBMClassifier(colsample_bytree= 0.7378703019867917,learning_rate= 0.007929963347654646,max_depth=5,min_child_weight= 0.05345076003503776,num_leaves= 20,subsample= 0.892939141154265)

#

clfs = {
    'sv' : svc,
    'xgb':xgb,
    'dtc':dtc,
    'lrc':lrc,
    'rfc':rfc
}

from sklearn.metrics import precision_score,accuracy_score
import joblib

def train_classifier(name, clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)
    filename = name+'.sav'
    joblib.dump(clf, filename)
    return accuracy,precision

def train(clfs,x_train,y_train,x_test,y_test):
    accuracy_scores = []
    precision_scores = []

    for name,clf in clfs.items():

        current_accuracy,current_precision = train_classifier(name, clf, x_train,y_train,x_test,y_test)

        print("For ",name)
        print("Accuracy - ",current_accuracy)
        print("Precision - ",current_precision)

        accuracy_scores.append(current_accuracy)
        precision_scores.append(current_precision)

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">8. Hyperparameter Tuning</div>"""

# Hyperparameter-tuning: Bayesian Optimization, bayes_opt
"""def lgbm_clf_bo(num_leaves, max_depth, learning_rate, min_child_weight, subsample, colsample_bytree):
    params_lgbm = {'objective': 'binary'}
    params_lgbm['num_leaves'] = round((2**round(max_depth))*num_leaves)
    params_lgbm['max_depth'] = round(max_depth)
    params_lgbm['learning_rate'] = learning_rate
    params_lgbm['min_child_weight'] = min_child_weight
    params_lgbm['subsample'] = subsample
    params_lgbm['colsample_bytree'] = colsample_bytree

    scores = cross_val_score(LGBMClassifier(random_state=12, **params_lgbm),
                             X_new, y_train, scoring='accuracy', cv=10).mean()
    return scores

# Set parameters distribution
params_lgbm ={
    'num_leaves':(0.5,0.9),
    'max_depth': (3, 15),
    'learning_rate': (0.005, 0.3),
    'min_child_weight':(1e-6, 1e-1),
    'subsample':(0.5, 1),
    'colsample_bytree':(0.5, 1)
}

# Run Bayesian Optimization
lgbm_bo = BayesianOptimization(lgbm_clf_bo, params_lgbm)
lgbm_bo.maximize(init_points=2, n_iter=20)"""

# Best hyperparameters
"""params_lgbm = lgbm_bo.max['params']
params_lgbm['max_depth'] = round(params_lgbm['max_depth'])
params_lgbm['num_leaves'] = round((2**round(params_lgbm['max_depth']))*params_lgbm['num_leaves'])
params_lgbm"""

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">9. Training Final Model</div>

from sklearn.ensemble import VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier


knn = KNeighborsClassifier()
cat = CatBoostClassifier(verbose=0)
lgb = LGBMClassifier()


ensemble = VotingClassifier(estimators=[('knn', knn),  ('cat', cat), ('lgb', lgb)], voting='soft',verbose=0)
ensemble_accuracy, ensemble_precision = train_classifier(ensemble, X_new, y_train, X_new_test, y_test)

print("For ensemble")
print("Accuracy - ", ensemble_accuracy)
print("Precision - ", ensemble_precision)"
"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor()
from sklearn.feature_selection import RFE

def feature_scores(x1_train,y1_train):
    clf = RandomForestRegressor()


    clf.fit(x1_train,y1_train)

    feature_scores = pd.Series(clf.feature_importances_, index=x1_train.columns).sort_values(ascending=False)
    return feature_scores

"""# <div style="border-radius:0px; border:#202Fd1 solid; padding: 15px; background-color: #2020d1; font-size:100%; text-align:center; color: #FFFFFF">10. Build Multi layer Graph</div>

[image.png](https://media.licdn.com/dms/image/D4E12AQHGcUY5OQkCuw/article-cover_image-shrink_600_2000/0/1671270073854?e=2147483647&v=beta&t=wG3rrat05emDAqJBol42VIv_T8kAuK0sTd3GVuTpogY)
"""

"""
    @Description Matrix 2D type checkin
    @Methods matrix_2d_test
    @Params matrix - 2D Matrix to check out
    @Return flag: True if it's a 2D Matrix anf False else
"""
def matrix_2d_test(matrix):
    flag = False
    if isinstance(matrix, list):
        if isinstance(matrix[0], list):
            intern_flag =True
            for row in matrix:
                if len(row) != len(matrix):
                    intern_flag = False
            flag = intern_flag
    return flag

"""
    @Description Entity nodes list generation
    @Methods generate_entity_nodes_list
    @Params data - a source of data which be a dataframe, series, list or 2D matrix
    @Return NODE_LIST
"""
def generate_entity_nodes_list(data):
    NB_ENTITY = 0
    NODE_LIST = []
    if isinstance(data, pd.DataFrame):
        NB_ENTITY = data.shape[0]
    if isinstance(data, pd.Series):
        NB_ENTITY = data.size
    """if isinstance(data, list):
        NB_ENTITY = len(data)
    if isinstance(data, list):
        if isinstance(data[0], list):
            if matrix_2d_test(data):"""
    NODE_LIST = [i for i in range(NB_ENTITY)]
    return NODE_LIST

#_DATA = pd.concat([X_new, X_new_test], axis=0)

import networkx as nx

def build_mlg(data, features):
    CRP_G = nx.DiGraph() # create an empty directed graph

    # build edges
    list_of_edges = []
    list_of_nodes = []
    LIST_OF_CUSTOMERS = data.index.values.tolist()
    LEN_OF_FEATURES = len(features)
    colors = [
        '#e6194b',
        '#ffe119',
        '#4363d8',
        '#f58231',
        '#911eb4',
        '#46f0f0',
        '#f032e6',
        '#bcf60c',
        '#fabebe',
        '#008080',
        '#e6beff',
        '#9a6324',
        '#fffac8',
        '#800000',
        '#aafdc9',
        '#808000',
        '#ffd8b1',
        '#000075',
        '#9cb44b',
        '#808080'

    ]
    for el in LIST_OF_CUSTOMERS: #fetch on custumers list
        # layer building
        for i in range(LEN_OF_FEATURES):
            # add nodes
            list_of_nodes.append(('C'+str(i)+'-U-'+str(el),{'color': 'g'}))
            for attr in features[i].tolist(): # fetch on home ownership encode values
                code = f"#{format(255-10*i, '02x')}{format(150+9*i, '02x')}{format(55+10*i, '02x')}"
                if int(data.loc[el,attr]) == 1: # check if exists relation between both
                    # bidirectional relation between home ownership and user
                    list_of_edges.append(('C'+str(i)+'-U-'+str(el),'C'+str(i)+'-M-'+attr, {'color': 'b'})) # add edge to list
                    list_of_edges.append(('C'+str(i)+'-M-'+attr, 'C'+str(i)+'-U-'+str(el), {'color': 'b'})) # add edge to list
                    # add nodes
                    list_of_nodes.append(('C'+str(i)+'-M-'+attr,{'color': colors[i]}))
            # add directed relation between user node from C1 and C2
            list_of_edges.append(('C'+str(i)+'-U-'+str(el),'C'+str(i+1 if i+1 < LEN_OF_FEATURES else i-1)+'-U-'+str(el), {'color': 'r'})) # add edge to list
            list_of_edges.append(('C'+str(i+1 if i+1 < LEN_OF_FEATURES else i-1 )+'-U-'+str(el), 'C'+str(i)+'-U-'+str(el), {'color': 'r'})) # add edge to list

    # add edges to the oriented graph
    print(list_of_nodes)
    CRP_G.add_nodes_from(list_of_nodes)
    CRP_G.add_edges_from(list_of_edges)

    # return the graph
    return CRP_G

# Create Credit Risk Prediction bipartite graph
x_train, x_test, y_train, y_test = test_train(DATA_OHE_LB_LBU_STDU_STDWO,'loan_status',test_size=0.2,random_state=12)
CRP_G = build_mlg(x_train.head(200), [OHE[0],OHE[1]])

#!pip install pyvis

#from pyvis.network import Network
#net = Network(notebook=True)
#net.from_nx(CRP_G)
#net.show('example.html')

# show
#colors = nx.get_edge_attributes(CRP_G,'color').values()
#colorsN = nx.get_node_attributes(CRP_G,'color').values()
#nx.draw(
#    CRP_G,
#    edge_color=colors,
#    node_color=colorsN,
#    with_labels=True)
#plt.show()

def find_indices(list_to_check, item_to_find):
    return [idx for idx, value in enumerate(list_to_check) if value == item_to_find]

def build_modalities_graph(X,Y,n):
    CRP_G = nx.DiGraph() # create an empty directed graph

    # join X and Y
    data = X.copy()
    data.astype('int')
    temp = data.copy()
    data["CLASS"] = Y.values
    data = data.head(n)

    # cast columns type to int
    data.astype('int')



    # build edges
    LIST_OF_CUSTOMERS = data.index.values.tolist()
    for el in LIST_OF_CUSTOMERS: #fetch on custumers list
        # find all columns belong to customer
        LINE = temp.loc[el,].values
        COLUMNS_BELONG = find_indices(LINE,1)

        # create edges
        COLUMNS = X.columns
        for i, col in enumerate(COLUMNS_BELONG): # fetch belong columns
            #print([i for i in range(i+1,len(COLUMNS_BELONG))], COLUMNS_BELONG)
            for j in range(i+1,len(COLUMNS_BELONG)): # fetch successor
                #if j < len(COLUMNS_BELONG) - 1: # check if it's the last column
                if CRP_G.has_edge(COLUMNS[col], COLUMNS[COLUMNS_BELONG[j]]):
                    # we added this one before, just increase the weight by one
                    CRP_G[COLUMNS[col]][COLUMNS[COLUMNS_BELONG[j]]]['weight'] += 1
                else:
                    # new edge. add with weight=1
                    CRP_G.add_edge(COLUMNS[col], COLUMNS[COLUMNS_BELONG[j]], weight=1)

                if CRP_G.has_edge(COLUMNS[COLUMNS_BELONG[j]], COLUMNS[col]):
                    # we added this one before, just increase the weight by one
                    CRP_G[COLUMNS[COLUMNS_BELONG[j]]][COLUMNS[col]]['weight'] += 1
                else:
                    # new edge. add with weight=1
                    CRP_G.add_edge(COLUMNS[COLUMNS_BELONG[j]], COLUMNS[col], weight=1)

            """if i == len(COLUMNS_BELONG) - 2: # check if it's the last column
                if CRP_G.has_edge(COLUMNS[col], COLUMNS[COLUMNS_BELONG[i+1]]):
                    # we added this one before, just increase the weight by one
                    CRP_G[COLUMNS[col]][COLUMNS[COLUMNS_BELONG[i+1]]]['weight'] += 1
                else:
                    # new edge. add with weight=1
                    CRP_G.add_edge(COLUMNS[col], COLUMNS[COLUMNS_BELONG[i+1]], weight=1)

                if CRP_G.has_edge(COLUMNS[COLUMNS_BELONG[i+1]], COLUMNS[col]):
                    # we added this one before, just increase the weight by one
                    CRP_G[COLUMNS[COLUMNS_BELONG[i+1]]][COLUMNS[col]]['weight'] += 1
                else:
                    # new edge. add with weight=1
                    CRP_G.add_edge(COLUMNS[COLUMNS_BELONG[i+1]], COLUMNS[col], weight=1)"""



            # add to class
            CLASS = data.loc[el,'CLASS']
            if CRP_G.has_edge(COLUMNS[col], CLASS):
                # we added this one before, just increase the weight by one
                CRP_G[COLUMNS[col]][CLASS]['weight'] += 1
            else:
                # new edge. add with weight=1
                CRP_G.add_edge(COLUMNS[col], CLASS, weight=1)

            if CRP_G.has_edge(CLASS, COLUMNS[col]):
                # we added this one before, just increase the weight by one
                CRP_G[CLASS][COLUMNS[col]]['weight'] += 1
            else:
                # new edge. add with weight=1
                CRP_G.add_edge(CLASS, COLUMNS[col], weight=1)




    # return the graph
    return CRP_G, data

MODALITIES_GRAPHE, datas = build_modalities_graph(x_train[np.concatenate((OHE[0],OHE[1]))], y_train, x_train.shape[0])

elarge = [(u, v) for (u, v, d) in MODALITIES_GRAPHE.edges(data=True) if d["weight"] > datas.shape[0] // 4]
esmall = [(u, v) for (u, v, d) in MODALITIES_GRAPHE.edges(data=True) if d["weight"] <= datas.shape[0] // 4]

pos = nx.spring_layout(MODALITIES_GRAPHE)  # positions for all nodes - seed for reproducibility

# nodes
nx.draw_networkx_nodes(MODALITIES_GRAPHE, pos, node_size=1000)

# edges
nx.draw_networkx_edges(MODALITIES_GRAPHE, pos, edgelist=elarge, width=6)
nx.draw_networkx_edges(
    MODALITIES_GRAPHE, pos, edgelist=esmall, width=6, alpha=0.5, edge_color="b", style="dashed"
)

# node labels
nx.draw_networkx_labels(MODALITIES_GRAPHE, pos, font_size=9, font_family="sans-serif")
# edge weight labels
edge_labels = nx.get_edge_attributes(MODALITIES_GRAPHE, "weight")
nx.draw_networkx_edge_labels(MODALITIES_GRAPHE, pos, edge_labels)

ax = plt.gca()
ax.margins(0.08)
plt.axis("off")
plt.tight_layout()
plt.show()

datas

"""#### Binome page rank"""

modal_combine = nx.pagerank(MODALITIES_GRAPHE, alpha=0.85)

bipart_combine = nx.pagerank(CRP_G, alpha=0.85)

"""
    @Description Get intra network nodes
    @Methods get_intra_node_label
    @Params graph - multilayer graph
    @Return labels - list of all intra nodes inside the graph
"""
def get_intra_node_label(graph):
  nodes = graph.nodes()
  intras = [k for k in nodes if '-M-' in k]
  return intras

"""
    @Description Get inter network nodes
    @Methods get_inter_node_label
    @Params graph - multilayer graph
    @Return labels - list of all inter nodes inside the graph
"""
def get_inter_node_label(graph):
  nodes = graph.nodes()
  inters = [k for k in nodes if '-U-' in k]
  return inters

def compute_personlization(node_list):
  personlized = dict()
  a = [ personlized.update({ k : 1/len(node_list) }) for k in node_list ]
  return personlized

# get intra page rank
bipart_intra_pagerank = nx.pagerank(CRP_G,personalization=compute_personlization(get_intra_node_label(CRP_G)), alpha=0.85)
# get inter page rank
bipart_inter_pagerank = nx.pagerank(CRP_G,personalization=compute_personlization(get_inter_node_label(CRP_G)), alpha=0.85)

bipart_intra_pagerank

bipart_inter_pagerank

# extract features based on 'person_home_ownership', 'loan_intent'
# for each borrowers get
import collections
# Number of borrowers with the same person_home_ownership
def get_number_of_borrowers_with_same_person_home_ownership(borrower, graph):
  edge = [(A,B) for (A,B) in graph.edges() if ('C0-M-' in B) and ('C0-U-' + str(borrower) == A)]
  #print(edge[0][1])
  edges = [(A,B) for (A,B) in graph.edges() if B == edge[0][1]]
  #print(edges)
  return [edges,len(edges) - 1]

#print(get_number_of_borrowers_with_same_person_home_ownership(0,CRP_G))

# Number of borrowers with the same loan_intent
def get_number_of_borrowers_with_same_loan_intent(borrower, graph):
  edge = [(A,B) for (A,B) in graph.edges() if ('C1-M-' in B) and ('C1-U-' + str(borrower) == A)]
  #print(edge[0][1])
  edges = [(A,B) for (A,B) in graph.edges() if B == edge[0][1]]
  #print(edges)
  return [edges,len(edges) - 1]

#print(get_number_of_borrowers_with_same_loan_intent(0,CRP_G))

# intra PR
def get_max_borrower_pr(pr):
  borrower = {}
  for key, val in pr.items():
    if '-U-' in key:
      borrower[int(key.split("-U-")[1])] = max(val, borrower[int(key.split("-U-")[1])]) if int(key.split("-U-")[1]) in borrower else val
  return [val for key, val in collections.OrderedDict(sorted(borrower.items())).items()]
# inter PR

# combine PR

# Number of borrowers with the same loan_intent
def get_number_of_borrowers_with_same_loan_intent_and_same_person_home_ownership(borrower, graph):
  edges_sho = get_number_of_borrowers_with_same_person_home_ownership(borrower, graph)[0]
  edges_sli = get_number_of_borrowers_with_same_loan_intent(borrower, graph)[0]

  just_sho_borrowers = [A[len(A)-1] for (A,B) in edges_sho]
  just_sli_borrowers = [A[len(A)-1] for (A,B) in edges_sli]


  just_sho_borrowers = set(just_sho_borrowers)
  just_sli_borrowers = set(just_sli_borrowers)

  intersection = just_sli_borrowers.intersection(just_sho_borrowers)
  #print("""
  #{}
  #------
  #{}
  #------
  #{}
  #""".format(edges_sho, edges_sli, intersection))
  return [list(intersection),len(intersection) - 1]

#print(get_number_of_borrowers_with_same_loan_intent_and_same_person_home_ownership(0,CRP_G))
# number of borrowers
def get_persons(graph):
  user_1_layer = [el  for el in graph.nodes() if 'C0-U-' in el]
  index_users = list(range(len(user_1_layer)))
  return index_users

extracts = {}
extracts['person_home_ownership_degree'] = [get_number_of_borrowers_with_same_person_home_ownership(index,CRP_G)[1] for index in get_persons(CRP_G)]
#print("----------")
extracts['person_loan_intent_degree'] = [get_number_of_borrowers_with_same_loan_intent(index,CRP_G)[1] for index in get_persons(CRP_G)]
#print("----------")
extracts['person_loan_intent_and_home_ownership_degree'] = [get_number_of_borrowers_with_same_loan_intent_and_same_person_home_ownership(index,CRP_G)[1] for index in get_persons(CRP_G)]
extracts['bipart_intra'] = get_max_borrower_pr(bipart_intra_pagerank)
extracts['bipart_inter'] = get_max_borrower_pr(bipart_inter_pagerank)
extracts['bipart_combine'] = get_max_borrower_pr(bipart_combine)

extracts['person_home_ownership_degree']

extracts['person_loan_intent_degree']

extracts['person_loan_intent_and_home_ownership_degree']

# standardization of new features
def standard_extraction(extracts, feats):
    for key in feats:
        extracts[key] = [el/max(extracts[key]) for el in extracts[key]]

standard_extraction(extracts, ['person_home_ownership_degree','person_loan_intent_degree', 'person_loan_intent_and_home_ownership_degree'])
print("""
  {}
  """.format(extracts))

def inject_features_extracted(data,features):
  dataframe = data.copy()
  for key, val in features.items():
    #print(f"{dataframe.shape[0]} --> {len(val)} : {key}")
    dataframe[key] = val
  return dataframe

#weight = nx.get_edge_attributes(MODALITIES_GRAPHE, "weight")
#weight

# train without extract features

# with statistic add variables
x_train_1, x_test_1, y_train_1, y_test_1 = test_train(DATA_OHE_LB_LBU_STDU_STDWO.head(200),'loan_status',test_size=0.2,random_state=12)
train(clfs,x_train_1,y_train_1,x_test_1,y_test_1)
print(feature_scores(x_train_1,y_train_1))

# without it
x1_train_1, x1_test_1, y1_train_1, y1_test_1 = test_train(DATA_ORIGIN.head(200),'loan_status',test_size=0.2,random_state=12)
train(clfs,x1_train_1,y1_train_1,x1_test_1,y1_test_1)
print(feature_scores(x1_train_1,y1_train_1))

# train with extract features
DATA_OHE_LB_LBU_STDU_STDWO_F = inject_features_extracted(DATA_OHE_LB_LBU_STDU_STDWO.head(200), extracts)
DATA_ORIGIN_F = inject_features_extracted(DATA_ORIGIN.head(200), extracts)
# with statistic add variables
x_train_2, x_test_2, y_train_2, y_test_2 = test_train(DATA_OHE_LB_LBU_STDU_STDWO_F,'loan_status',test_size=0.2,random_state=12)
train(clfs,x_train_2,y_train_2,x_test_2,y_test_2)
print(feature_scores(x_train_2,y_train_2))

# without it
x1_train_2, x1_test_2, y1_train_2, y1_test_2 = test_train(DATA_ORIGIN_F,'loan_status',test_size=0.2,random_state=12)
train(clfs,x1_train_2,y1_train_2,x1_test_2,y1_test_2)
print(feature_scores(x1_train_2,y1_train_2))

